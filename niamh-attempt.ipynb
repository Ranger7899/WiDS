{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1213, 19930)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning and Concatenation\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided Excel files\n",
    "categorical_metadata_path = 'widsdatathon2025/TRAIN/TRAIN_CATEGORICAL_METADATA.xlsx'\n",
    "quantitative_metadata_path = 'widsdatathon2025/TRAIN/TRAIN_QUANTITATIVE_METADATA.xlsx'\n",
    "functional_path = 'widsdatathon2025/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv'\n",
    "solutions_path = 'widsdatathon2025/TRAIN/TRAINING_SOLUTIONS.xlsx'\n",
    "\n",
    "categorical_metadata = pd.read_excel(categorical_metadata_path)\n",
    "quantitative_metadata = pd.read_excel(quantitative_metadata_path)\n",
    "function_data = pd.read_csv(functional_path)\n",
    "solutions = pd.read_excel(solutions_path)\n",
    "\n",
    "# Check the first few rows of each to inspect their structure\n",
    "categorical_metadata.head(), quantitative_metadata.head(), function_data.head(), solutions.head()\n",
    "\n",
    "# Merge the dataframes by 'participant_id'\n",
    "merged_data = pd.merge(categorical_metadata, quantitative_metadata, on='participant_id', how='outer')\n",
    "merged_data = pd.merge(merged_data, function_data, on='participant_id', how='outer')\n",
    "merged_data = pd.merge(merged_data, solutions, on='participant_id', how='outer')\n",
    "\n",
    "merged_data.head()\n",
    "merged_data.to_csv('merged_data.csv', index=False)\n",
    "merged_data.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was SUPER lazy and just deleted any columns with any NAs, a later problem if you ask me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of NaN values\n",
    "# -------------------------\n",
    "\n",
    "# Load in the megred data\n",
    "merged_data = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# Check for NaN values in the merged data\n",
    "merged_data.isnull().sum()\n",
    "\n",
    "#List the columns with NaN values\n",
    "columns_with_nan = merged_data.columns[merged_data.isna().any()].tolist()\n",
    "columns_with_nan\n",
    "\n",
    "# Drop columns with NaN values\n",
    "merged_data = merged_data.dropna(axis=1)\n",
    "\n",
    "# Check the dimensions of the data\n",
    "merged_data.shape\n",
    "\n",
    "# Convert the merged data to a CSV file\n",
    "merged_data.to_csv('merged_data_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((970, 19925), (243, 19925), (970, 2), (243, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the merged CSV file (assuming it's already merged)\n",
    "merged_file_path = 'merged_data_cleaned.csv'  # Path to your merged file\n",
    "merged_data = pd.read_csv(merged_file_path)\n",
    "\n",
    "# Select features and target columns\n",
    "target_columns = ['Sex_F', 'ADHD_Outcome']\n",
    "feature_columns = [col for col in merged_data.columns if col not in target_columns and col != 'participant_id']\n",
    "\n",
    "# Features and targets\n",
    "X = merged_data[feature_columns]\n",
    "y = merged_data[target_columns]\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the data\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- Custom Dataset for Multi-Task Learning ---\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, target_cols):\n",
    "        self.y = df[target_cols].values.astype(np.float32)\n",
    "        self.X = df.drop(columns=target_cols + ['participant_id']).values.astype(np.float32)  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]  # y is a vector of targets\n",
    "\n",
    "# --- Custom Weighted Loss Function ---\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, target, sex, adhd):\n",
    "        # Assign a weight of 2 for female ADHD cases (Sex_F=1, ADHD_Outcome=1)\n",
    "        weights = torch.ones_like(target[:, 0])  # Default weight is 1, shape (batch_size,)\n",
    "        weights[(sex == 1) & (adhd == 1)] = 2  # Apply weight of 2 for female ADHD cases\n",
    "        \n",
    "        # Compute the loss for each task\n",
    "        loss_sex = nn.BCELoss(reduction='none')(preds[:, 0], target[:, 0])  # Loss for Sex_F\n",
    "        loss_adhd = nn.BCELoss(reduction='none')(preds[:, 1], target[:, 1])  # Loss for ADHD_Outcome\n",
    "        \n",
    "        # Apply the weights to each task's loss\n",
    "        weighted_loss_sex = loss_sex * weights  # Apply weight for Sex_F loss\n",
    "        weighted_loss_adhd = loss_adhd * weights  # Apply weight for ADHD_Outcome loss\n",
    "\n",
    "        return (weighted_loss_sex.mean() + weighted_loss_adhd.mean()) / 2\n",
    "\n",
    "# --- Multi-Task Neural Network ---\n",
    "class MultiTaskNN(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "        # Shared layers with dropout\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),  # Dropout added here\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(dropout_rate)  # Dropout added here\n",
    "        )\n",
    "        # Separate heads for each task\n",
    "        self.sex_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.adhd_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared_rep = self.shared(x)\n",
    "        sex_pred = self.sex_head(shared_rep)\n",
    "        adhd_pred = self.adhd_head(shared_rep)\n",
    "        return torch.cat([sex_pred, adhd_pred], dim=1)  # output size [batch_size, 2]\n",
    "\n",
    "# --- F1 Score Calculation with Weights ---\n",
    "def weighted_f1_score(preds, target, sex, adhd):\n",
    "    weights = np.ones_like(target[:, 0])  # Default weight is 1 for both tasks\n",
    "    weights[(sex == 1) & (adhd == 1)] = 2  # Apply weight of 2 for female ADHD cases\n",
    "    \n",
    "    # Calculate F1 score for both tasks\n",
    "    f1_sex = f1_score(target[:, 0], (preds[:, 0] > 0.5).astype(int), sample_weight=weights)\n",
    "    f1_adhd = f1_score(target[:, 1], (preds[:, 1] > 0.5).astype(int), sample_weight=weights)\n",
    "    \n",
    "    return (f1_sex + f1_adhd) / 2\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_model(df, target_cols, num_epochs=100, batch_size=64, learning_rate=0.0001, weight_decay=0.01, dropout_rate=0.2):\n",
    "    # Set device (this should be defined before using it)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Split the data into training and validation sets (80-20 split)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = MultiTaskDataset(train_df, target_cols)\n",
    "    val_dataset = MultiTaskDataset(val_df, target_cols)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    input_dim = train_df.drop(columns=target_cols + ['participant_id']).shape[1]\n",
    "    model = MultiTaskNN(input_dim, dropout_rate).to(device)\n",
    "    \n",
    "    # Initialize custom weighted loss\n",
    "    criterion = WeightedBCELoss()\n",
    "\n",
    "    # Define optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_f1 = 0  # For tracking best F1 score\n",
    "    model_weights_per_epoch = []  # Store model weights at each epoch\n",
    "    val_f1_scores = []  # Store F1 scores for validation\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            sex = y_batch[:, 0]  # Extract Sex_F values\n",
    "            adhd = y_batch[:, 1]  # Extract ADHD_Outcome values\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            \n",
    "            # Calculate weighted loss\n",
    "            loss = criterion(preds, y_batch, sex, adhd)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(y_batch.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        \n",
    "        # Calculate weighted F1 score for both tasks\n",
    "        f1_score_avg = weighted_f1_score(all_preds, all_targets, all_targets[:, 0], all_targets[:, 1])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val Weighted F1 Score: {f1_score_avg:.4f}\")\n",
    "        \n",
    "        # Store model weights and validation F1 score\n",
    "        model_weights_per_epoch.append(model.state_dict())  # Save weights\n",
    "        val_f1_scores.append(f1_score_avg)  # Save validation score\n",
    "\n",
    "    # After training, find the epoch with the best validation F1 score\n",
    "    best_epoch = np.argmax(val_f1_scores)  # Get the index of the best F1 score\n",
    "    print(f\"Best epoch: {best_epoch + 1} with F1 Score: {val_f1_scores[best_epoch]:.4f}\")\n",
    "    \n",
    "    # Reload the best model weights\n",
    "    model.load_state_dict(model_weights_per_epoch[best_epoch])  # Load best model weights\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Val Weighted F1 Score: 0.4150\n",
      "Epoch 2/100 - Val Weighted F1 Score: 0.6516\n",
      "Epoch 3/100 - Val Weighted F1 Score: 0.6507\n",
      "Epoch 4/100 - Val Weighted F1 Score: 0.6975\n",
      "Epoch 5/100 - Val Weighted F1 Score: 0.7101\n",
      "Epoch 6/100 - Val Weighted F1 Score: 0.7152\n",
      "Epoch 7/100 - Val Weighted F1 Score: 0.7189\n",
      "Epoch 8/100 - Val Weighted F1 Score: 0.7140\n",
      "Epoch 9/100 - Val Weighted F1 Score: 0.7227\n",
      "Epoch 10/100 - Val Weighted F1 Score: 0.7075\n",
      "Epoch 11/100 - Val Weighted F1 Score: 0.4150\n",
      "Epoch 12/100 - Val Weighted F1 Score: 0.6275\n",
      "Epoch 13/100 - Val Weighted F1 Score: 0.7265\n",
      "Epoch 14/100 - Val Weighted F1 Score: 0.7150\n",
      "Epoch 15/100 - Val Weighted F1 Score: 0.4240\n",
      "Epoch 16/100 - Val Weighted F1 Score: 0.6233\n",
      "Epoch 17/100 - Val Weighted F1 Score: 0.7247\n",
      "Epoch 18/100 - Val Weighted F1 Score: 0.1792\n",
      "Epoch 19/100 - Val Weighted F1 Score: 0.4333\n",
      "Epoch 20/100 - Val Weighted F1 Score: 0.2641\n",
      "Epoch 21/100 - Val Weighted F1 Score: 0.3013\n",
      "Epoch 22/100 - Val Weighted F1 Score: 0.2400\n",
      "Epoch 23/100 - Val Weighted F1 Score: 0.7149\n",
      "Epoch 24/100 - Val Weighted F1 Score: 0.4150\n",
      "Epoch 25/100 - Val Weighted F1 Score: 0.4836\n",
      "Epoch 26/100 - Val Weighted F1 Score: 0.6508\n",
      "Epoch 27/100 - Val Weighted F1 Score: 0.7515\n",
      "Epoch 28/100 - Val Weighted F1 Score: 0.3876\n",
      "Epoch 29/100 - Val Weighted F1 Score: 0.4625\n",
      "Epoch 30/100 - Val Weighted F1 Score: 0.3890\n",
      "Epoch 31/100 - Val Weighted F1 Score: 0.4024\n",
      "Epoch 32/100 - Val Weighted F1 Score: 0.7195\n",
      "Epoch 33/100 - Val Weighted F1 Score: 0.6325\n",
      "Epoch 34/100 - Val Weighted F1 Score: 0.7173\n",
      "Epoch 35/100 - Val Weighted F1 Score: 0.7152\n",
      "Epoch 36/100 - Val Weighted F1 Score: 0.6006\n",
      "Epoch 37/100 - Val Weighted F1 Score: 0.5670\n",
      "Epoch 38/100 - Val Weighted F1 Score: 0.6813\n",
      "Epoch 39/100 - Val Weighted F1 Score: 0.4408\n",
      "Epoch 40/100 - Val Weighted F1 Score: 0.7253\n",
      "Epoch 41/100 - Val Weighted F1 Score: 0.7325\n",
      "Epoch 42/100 - Val Weighted F1 Score: 0.3251\n",
      "Epoch 43/100 - Val Weighted F1 Score: 0.2509\n",
      "Epoch 44/100 - Val Weighted F1 Score: 0.2636\n",
      "Epoch 45/100 - Val Weighted F1 Score: 0.6143\n",
      "Epoch 46/100 - Val Weighted F1 Score: 0.6319\n",
      "Epoch 47/100 - Val Weighted F1 Score: 0.7313\n",
      "Epoch 48/100 - Val Weighted F1 Score: 0.5180\n",
      "Epoch 49/100 - Val Weighted F1 Score: 0.2288\n",
      "Epoch 50/100 - Val Weighted F1 Score: 0.2536\n",
      "Epoch 51/100 - Val Weighted F1 Score: 0.6925\n",
      "Epoch 52/100 - Val Weighted F1 Score: 0.6126\n",
      "Epoch 53/100 - Val Weighted F1 Score: 0.6675\n",
      "Epoch 54/100 - Val Weighted F1 Score: 0.7339\n",
      "Epoch 55/100 - Val Weighted F1 Score: 0.7353\n",
      "Epoch 56/100 - Val Weighted F1 Score: 0.6564\n",
      "Epoch 57/100 - Val Weighted F1 Score: 0.7549\n",
      "Epoch 58/100 - Val Weighted F1 Score: 0.7183\n",
      "Epoch 59/100 - Val Weighted F1 Score: 0.7140\n",
      "Epoch 60/100 - Val Weighted F1 Score: 0.6430\n",
      "Epoch 61/100 - Val Weighted F1 Score: 0.4483\n",
      "Epoch 62/100 - Val Weighted F1 Score: 0.7330\n",
      "Epoch 63/100 - Val Weighted F1 Score: 0.4963\n",
      "Epoch 64/100 - Val Weighted F1 Score: 0.3863\n",
      "Epoch 65/100 - Val Weighted F1 Score: 0.4962\n",
      "Epoch 66/100 - Val Weighted F1 Score: 0.5244\n",
      "Epoch 67/100 - Val Weighted F1 Score: 0.4297\n",
      "Epoch 68/100 - Val Weighted F1 Score: 0.6372\n",
      "Epoch 69/100 - Val Weighted F1 Score: 0.7304\n",
      "Epoch 70/100 - Val Weighted F1 Score: 0.7391\n",
      "Epoch 71/100 - Val Weighted F1 Score: 0.5795\n",
      "Epoch 72/100 - Val Weighted F1 Score: 0.4105\n",
      "Epoch 73/100 - Val Weighted F1 Score: 0.5043\n",
      "Epoch 74/100 - Val Weighted F1 Score: 0.7386\n",
      "Epoch 75/100 - Val Weighted F1 Score: 0.7140\n",
      "Epoch 76/100 - Val Weighted F1 Score: 0.7375\n",
      "Epoch 77/100 - Val Weighted F1 Score: 0.3525\n",
      "Epoch 78/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 79/100 - Val Weighted F1 Score: 0.1078\n",
      "Epoch 80/100 - Val Weighted F1 Score: 0.7584\n",
      "Epoch 81/100 - Val Weighted F1 Score: 0.7160\n",
      "Epoch 82/100 - Val Weighted F1 Score: 0.7152\n",
      "Epoch 83/100 - Val Weighted F1 Score: 0.7152\n",
      "Epoch 84/100 - Val Weighted F1 Score: 0.4309\n",
      "Epoch 85/100 - Val Weighted F1 Score: 0.4267\n",
      "Epoch 86/100 - Val Weighted F1 Score: 0.7149\n",
      "Epoch 87/100 - Val Weighted F1 Score: 0.7335\n",
      "Epoch 88/100 - Val Weighted F1 Score: 0.4265\n",
      "Epoch 89/100 - Val Weighted F1 Score: 0.1558\n",
      "Epoch 90/100 - Val Weighted F1 Score: 0.6894\n",
      "Epoch 91/100 - Val Weighted F1 Score: 0.5268\n",
      "Epoch 92/100 - Val Weighted F1 Score: 0.4435\n",
      "Epoch 93/100 - Val Weighted F1 Score: 0.4255\n",
      "Epoch 94/100 - Val Weighted F1 Score: 0.7222\n",
      "Epoch 95/100 - Val Weighted F1 Score: 0.7152\n",
      "Epoch 96/100 - Val Weighted F1 Score: 0.7560\n",
      "Epoch 97/100 - Val Weighted F1 Score: 0.4150\n",
      "Epoch 98/100 - Val Weighted F1 Score: 0.4163\n",
      "Epoch 99/100 - Val Weighted F1 Score: 0.4172\n",
      "Epoch 100/100 - Val Weighted F1 Score: 0.7210\n",
      "Best epoch: 80 with F1 Score: 0.7584\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned and merged dataset\n",
    "merged_data_cleaned = pd.read_csv('merged_data_cleaned.csv') \n",
    "\n",
    "# Define the target columns\n",
    "target_cols = ['Sex_F', 'ADHD_Outcome']  # Specify target columns\n",
    "\n",
    "\n",
    "# Train the model using the train_model function\n",
    "trained_model = train_model(merged_data_cleaned, target_cols, num_epochs=100, batch_size=64, learning_rate=0.00005, weight_decay=0.03, dropout_rate=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realised that I forgot to scale or standardise the data in any way at all the first run, so here is attempt 2. It is better! Marginally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/v1gpfmxd54d6bqwl7msx6jb40000gn/T/ipykernel_41539/3625712125.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  quant_data_cleaned[continuous_cols_quant] = scaler_quant.fit_transform(quant_data_cleaned[continuous_cols_quant])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the fMRI data (replace with your correct path)\n",
    "fmri_data_path = 'widsdatathon2025/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv'  # Replace with actual file path\n",
    "fmri_data = pd.read_csv(fmri_data_path)\n",
    "\n",
    "# Step 2: Separate the participant ID column and the fMRI data columns\n",
    "participant_id_fmri = fmri_data.iloc[:, 0]  # Assuming the first column is participant ID\n",
    "fmri_data_columns = fmri_data.iloc[:, 1:]  # All columns except the first (fMRI data)\n",
    "\n",
    "# Step 3: Apply StandardScaler to the fMRI data\n",
    "scaler = StandardScaler()\n",
    "scaled_fmri_data = scaler.fit_transform(fmri_data_columns)\n",
    "\n",
    "# Step 4: Convert the scaled data back to a DataFrame (optional, for easier handling)\n",
    "scaled_fmri_data_df = pd.DataFrame(scaled_fmri_data, columns=fmri_data_columns.columns)\n",
    "\n",
    "# Step 5: Concatenate the participant ID back with the scaled fMRI data\n",
    "final_fmri_data = pd.concat([participant_id_fmri, scaled_fmri_data_df], axis=1)\n",
    "\n",
    "# Step 6: Load the quantitative metadata (replace with the correct file path)\n",
    "quant_data_path = 'widsdatathon2025/TRAIN/TRAIN_QUANTITATIVE_METADATA.xlsx'  # Replace with actual file path\n",
    "quant_data = pd.read_excel(quant_data_path)\n",
    "\n",
    "# Step 7: Select only the necessary columns (keep all quantitative data columns)\n",
    "selected_columns_quant = [\n",
    "    'participant_id', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID',\n",
    "    'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "    'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan'\n",
    "]\n",
    "quant_data_selected = quant_data[selected_columns_quant]\n",
    "\n",
    "# Step 8: Drop columns with NA values (if any)\n",
    "quant_data_cleaned = quant_data_selected.dropna(axis=1, how='any')  # Drop columns with any NA values\n",
    "\n",
    "# Step 9: Apply Standard Scaling to the continuous data (excluding participant_id)\n",
    "continuous_cols_quant = quant_data_cleaned.columns[1:]  # Exclude participant_id from scaling\n",
    "scaler_quant = StandardScaler()\n",
    "quant_data_cleaned[continuous_cols_quant] = scaler_quant.fit_transform(quant_data_cleaned[continuous_cols_quant])\n",
    "\n",
    "# Step 10: Load the categorical metadata (replace with the correct file path)\n",
    "categorical_data_path = 'widsdatathon2025/TRAIN/TRAIN_CATEGORICAL_METADATA.xlsx'  # Replace with actual file path\n",
    "categorical_data = pd.read_excel(categorical_data_path)\n",
    "\n",
    "# Step 11: Select the necessary columns for one-hot encoding\n",
    "selected_columns_categorical = [\n",
    "    'participant_id', 'Basic_Demos_Enroll_Year', 'Basic_Demos_Study_Site', 'PreInt_Demos_Fam_Child_Ethnicity',\n",
    "    'PreInt_Demos_Fam_Child_Race', 'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ',\n",
    "    'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ'\n",
    "]\n",
    "categorical_data_selected = categorical_data[selected_columns_categorical]\n",
    "\n",
    "# Step 12: Drop columns with NA values (if any)\n",
    "categorical_data_cleaned = categorical_data_selected.dropna(axis=1, how='any')  # Drop columns with any NA values\n",
    "\n",
    "# Step 13: One-hot encode categorical columns (excluding 'participant_id')\n",
    "categorical_columns = categorical_data_cleaned.columns[1:]  # Exclude 'participant_id'\n",
    "categorical_data_encoded = pd.get_dummies(categorical_data_cleaned, columns=categorical_columns)\n",
    "\n",
    "# Step 14: Concatenate the cleaned fMRI data, the cleaned quantitative data, and the encoded categorical data\n",
    "full_data = pd.merge(final_fmri_data, quant_data_cleaned, on='participant_id', how='inner')\n",
    "full_data = pd.merge(full_data, categorical_data_encoded, on='participant_id', how='inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Val Weighted F1 Score: 0.7441\n",
      "Epoch 2/100 - Val Weighted F1 Score: 0.7276\n",
      "Epoch 3/100 - Val Weighted F1 Score: 0.7445\n",
      "Epoch 4/100 - Val Weighted F1 Score: 0.7474\n",
      "Epoch 5/100 - Val Weighted F1 Score: 0.7548\n",
      "Epoch 6/100 - Val Weighted F1 Score: 0.7606\n",
      "Epoch 7/100 - Val Weighted F1 Score: 0.7609\n",
      "Epoch 8/100 - Val Weighted F1 Score: 0.7668\n",
      "Epoch 9/100 - Val Weighted F1 Score: 0.7650\n",
      "Epoch 10/100 - Val Weighted F1 Score: 0.7579\n",
      "Epoch 11/100 - Val Weighted F1 Score: 0.7680\n",
      "Epoch 12/100 - Val Weighted F1 Score: 0.7667\n",
      "Epoch 13/100 - Val Weighted F1 Score: 0.7667\n",
      "Epoch 14/100 - Val Weighted F1 Score: 0.7666\n",
      "Epoch 15/100 - Val Weighted F1 Score: 0.7692\n",
      "Epoch 16/100 - Val Weighted F1 Score: 0.7744\n",
      "Epoch 17/100 - Val Weighted F1 Score: 0.7688\n",
      "Epoch 18/100 - Val Weighted F1 Score: 0.7633\n",
      "Epoch 19/100 - Val Weighted F1 Score: 0.7771\n",
      "Epoch 20/100 - Val Weighted F1 Score: 0.7820\n",
      "Epoch 21/100 - Val Weighted F1 Score: 0.7807\n",
      "Epoch 22/100 - Val Weighted F1 Score: 0.7455\n",
      "Epoch 23/100 - Val Weighted F1 Score: 0.7805\n",
      "Epoch 24/100 - Val Weighted F1 Score: 0.7685\n",
      "Epoch 25/100 - Val Weighted F1 Score: 0.7557\n",
      "Epoch 26/100 - Val Weighted F1 Score: 0.7481\n",
      "Epoch 27/100 - Val Weighted F1 Score: 0.7451\n",
      "Epoch 28/100 - Val Weighted F1 Score: 0.7760\n",
      "Epoch 29/100 - Val Weighted F1 Score: 0.7403\n",
      "Epoch 30/100 - Val Weighted F1 Score: 0.7254\n",
      "Epoch 31/100 - Val Weighted F1 Score: 0.7658\n",
      "Epoch 32/100 - Val Weighted F1 Score: 0.7551\n",
      "Epoch 33/100 - Val Weighted F1 Score: 0.7594\n",
      "Epoch 34/100 - Val Weighted F1 Score: 0.7659\n",
      "Epoch 35/100 - Val Weighted F1 Score: 0.7682\n",
      "Epoch 36/100 - Val Weighted F1 Score: 0.7209\n",
      "Epoch 37/100 - Val Weighted F1 Score: 0.7172\n",
      "Epoch 38/100 - Val Weighted F1 Score: 0.7272\n",
      "Epoch 39/100 - Val Weighted F1 Score: 0.7149\n",
      "Epoch 40/100 - Val Weighted F1 Score: 0.7206\n",
      "Epoch 41/100 - Val Weighted F1 Score: 0.7120\n",
      "Epoch 42/100 - Val Weighted F1 Score: 0.7302\n",
      "Epoch 43/100 - Val Weighted F1 Score: 0.7324\n",
      "Epoch 44/100 - Val Weighted F1 Score: 0.7261\n",
      "Epoch 45/100 - Val Weighted F1 Score: 0.6741\n",
      "Epoch 46/100 - Val Weighted F1 Score: 0.7097\n",
      "Epoch 47/100 - Val Weighted F1 Score: 0.6896\n",
      "Epoch 48/100 - Val Weighted F1 Score: 0.6802\n",
      "Epoch 49/100 - Val Weighted F1 Score: 0.7175\n",
      "Epoch 50/100 - Val Weighted F1 Score: 0.7326\n",
      "Epoch 51/100 - Val Weighted F1 Score: 0.6564\n",
      "Epoch 52/100 - Val Weighted F1 Score: 0.6786\n",
      "Epoch 53/100 - Val Weighted F1 Score: 0.6431\n",
      "Epoch 54/100 - Val Weighted F1 Score: 0.7297\n",
      "Epoch 55/100 - Val Weighted F1 Score: 0.6552\n",
      "Epoch 56/100 - Val Weighted F1 Score: 0.6514\n",
      "Epoch 57/100 - Val Weighted F1 Score: 0.7080\n",
      "Epoch 58/100 - Val Weighted F1 Score: 0.7001\n",
      "Epoch 59/100 - Val Weighted F1 Score: 0.6424\n",
      "Epoch 60/100 - Val Weighted F1 Score: 0.6970\n",
      "Epoch 61/100 - Val Weighted F1 Score: 0.6744\n",
      "Epoch 62/100 - Val Weighted F1 Score: 0.6435\n",
      "Epoch 63/100 - Val Weighted F1 Score: 0.7115\n",
      "Epoch 64/100 - Val Weighted F1 Score: 0.7108\n",
      "Epoch 65/100 - Val Weighted F1 Score: 0.7108\n",
      "Epoch 66/100 - Val Weighted F1 Score: 0.7332\n",
      "Epoch 67/100 - Val Weighted F1 Score: 0.6861\n",
      "Epoch 68/100 - Val Weighted F1 Score: 0.6828\n",
      "Epoch 69/100 - Val Weighted F1 Score: 0.6918\n",
      "Epoch 70/100 - Val Weighted F1 Score: 0.6763\n",
      "Epoch 71/100 - Val Weighted F1 Score: 0.7025\n",
      "Epoch 72/100 - Val Weighted F1 Score: 0.6742\n",
      "Epoch 73/100 - Val Weighted F1 Score: 0.6642\n",
      "Epoch 74/100 - Val Weighted F1 Score: 0.7071\n",
      "Epoch 75/100 - Val Weighted F1 Score: 0.6410\n",
      "Epoch 76/100 - Val Weighted F1 Score: 0.6459\n",
      "Epoch 77/100 - Val Weighted F1 Score: 0.6574\n",
      "Epoch 78/100 - Val Weighted F1 Score: 0.6930\n",
      "Epoch 79/100 - Val Weighted F1 Score: 0.6521\n",
      "Epoch 80/100 - Val Weighted F1 Score: 0.7017\n",
      "Epoch 81/100 - Val Weighted F1 Score: 0.6932\n",
      "Epoch 82/100 - Val Weighted F1 Score: 0.6616\n",
      "Epoch 83/100 - Val Weighted F1 Score: 0.6377\n",
      "Epoch 84/100 - Val Weighted F1 Score: 0.6180\n",
      "Epoch 85/100 - Val Weighted F1 Score: 0.5949\n",
      "Epoch 86/100 - Val Weighted F1 Score: 0.6588\n",
      "Epoch 87/100 - Val Weighted F1 Score: 0.6634\n",
      "Epoch 88/100 - Val Weighted F1 Score: 0.6404\n",
      "Epoch 89/100 - Val Weighted F1 Score: 0.6311\n",
      "Epoch 90/100 - Val Weighted F1 Score: 0.6396\n",
      "Epoch 91/100 - Val Weighted F1 Score: 0.6684\n",
      "Epoch 92/100 - Val Weighted F1 Score: 0.6701\n",
      "Epoch 93/100 - Val Weighted F1 Score: 0.6680\n",
      "Epoch 94/100 - Val Weighted F1 Score: 0.6119\n",
      "Epoch 95/100 - Val Weighted F1 Score: 0.6067\n",
      "Epoch 96/100 - Val Weighted F1 Score: 0.6736\n",
      "Epoch 97/100 - Val Weighted F1 Score: 0.6634\n",
      "Epoch 98/100 - Val Weighted F1 Score: 0.6185\n",
      "Epoch 99/100 - Val Weighted F1 Score: 0.6996\n",
      "Epoch 100/100 - Val Weighted F1 Score: 0.6707\n",
      "Best epoch: 20 with F1 Score: 0.7820\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Ensure the target data is loaded\n",
    "target_file_path = 'widsdatathon2025/TRAIN/TRAINING_SOLUTIONS.xlsx'  # Replace with the actual path to the target data\n",
    "target_data = pd.read_excel(target_file_path)\n",
    "\n",
    "# Step 2: Merge the feature data (already in full_data) with the target data based on participant_id\n",
    "data = pd.merge(full_data, target_data, on='participant_id', how='inner')\n",
    "\n",
    "# Step 3: Specify the target columns\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "# Step 4: Run the model training\n",
    "trained_model = train_model(data, target_cols, num_epochs=100, batch_size=64, learning_rate=0.0001, weight_decay=0.01, dropout_rate=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for my sanity's sake, I tested it against a logistic regression (took like 5 mins with my lab partner, ChatGPT)\n",
    "It's not better, thank god - xoxo Niamh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for ADHD_Outcome: 0.9725\n",
      "F1 Score for Sex_F: 0.4865\n",
      "Final Averaged F1 Score: 0.7295\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Merge target variables with feature data (assuming full_data is already prepared and includes both features and participant_id)\n",
    "target_file_path = 'widsdatathon2025/TRAIN/TRAINING_SOLUTIONS.xlsx'  # Replace with the actual path to the target data\n",
    "target_data = pd.read_excel(target_file_path)\n",
    "\n",
    "# Merge the target data with the full dataset\n",
    "data_with_targets = pd.merge(full_data, target_data, on='participant_id', how='inner')\n",
    "\n",
    "# Step 2: Specify the target columns\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "# Step 3: Split the data into features (X) and targets (y)\n",
    "X = data_with_targets.drop(columns=['participant_id'] + target_cols)  # Features\n",
    "y = data_with_targets[target_cols]  # Targets\n",
    "\n",
    "# Step 4: Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Standardize the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Train Logistic Regression models for both target variables\n",
    "model_adhd = LogisticRegression(max_iter=1000)\n",
    "model_sex = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the models for ADHD_Outcome and Sex_F\n",
    "model_adhd.fit(X_train_scaled, y_train['ADHD_Outcome'])\n",
    "model_sex.fit(X_train_scaled, y_train['Sex_F'])\n",
    "\n",
    "# Step 7: Make predictions on the test set\n",
    "y_pred_adhd = model_adhd.predict(X_test_scaled)\n",
    "y_pred_sex = model_sex.predict(X_test_scaled)\n",
    "\n",
    "# Step 8: Calculate the F1 score for both targets with weighted averaging\n",
    "# Apply weight of 2 for Female ADHD cases (ADHD_Outcome=1, Sex_F=1), else weight of 1\n",
    "\n",
    "# Calculate weights for ADHD_Outcome\n",
    "weights_adhd = (y_test['Sex_F'] == 1) & (y_test['ADHD_Outcome'] == 1)  # Weight of 2 for female ADHD cases\n",
    "f1_adhd = f1_score(y_test['ADHD_Outcome'], y_pred_adhd, average='weighted', sample_weight=weights_adhd)\n",
    "\n",
    "# Calculate weights for Sex_F\n",
    "weights_sex = (y_test['Sex_F'] == 1) & (y_test['ADHD_Outcome'] == 1)  # Weight of 2 for female ADHD cases\n",
    "f1_sex = f1_score(y_test['Sex_F'], y_pred_sex, average='weighted', sample_weight=weights_sex)\n",
    "\n",
    "# Step 9: Average the F1 scores for both tasks\n",
    "final_f1_score = (f1_adhd + f1_sex) / 2\n",
    "\n",
    "# Step 10: Output the F1 scores\n",
    "print(f\"F1 Score for ADHD_Outcome: {f1_adhd:.4f}\")\n",
    "print(f\"F1 Score for Sex_F: {f1_sex:.4f}\")\n",
    "print(f\"Final Averaged F1 Score: {final_f1_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
