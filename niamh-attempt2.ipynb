{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2\n",
    "\n",
    "This is a second and cleaner attempt. The main difference this time is that instead of just dropping the NA columns I use simple imputation with mean (could have used median, but it's a longer word to type). You do not have to run the first two code blocks - I have uploaded the imputed .csv files for the train and test data sets separately. \n",
    "\n",
    "Steps in this code:\n",
    "1. Load the data\n",
    "2. Imputation\n",
    "3. Create the model\n",
    "4. Train the model on the training data\n",
    "5. Predict and make .csv\n",
    "6. Win lots and lots of money\n",
    "\n",
    "Things that could probably make this better:\n",
    "- Dimensionality reduction on the fMRI data (needs scaling)\n",
    "- Iteratively work through the ann options to find the best model (can def code this and make it automatic, not like me manually messing with the numbers)\n",
    "    - The code I'm messing with at the bottom involves this bit to see if I can improve the bad f1 score I got with default values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed Training Data:\n",
      "   0throw_1thcolumn  0throw_2thcolumn  0throw_3thcolumn  0throw_4thcolumn  \\\n",
      "0          0.093473          0.146902          0.067893          0.015141   \n",
      "1          0.029580          0.179323          0.112933          0.038291   \n",
      "2         -0.051580          0.139734          0.068295          0.046991   \n",
      "3          0.016273          0.204702          0.115980          0.043103   \n",
      "4          0.065771          0.098714          0.097604          0.112988   \n",
      "\n",
      "   0throw_5thcolumn  0throw_6thcolumn  0throw_7thcolumn  0throw_8thcolumn  \\\n",
      "0          0.070221          0.063997          0.055382         -0.035335   \n",
      "1          0.104899          0.064250          0.008488          0.077505   \n",
      "2          0.111085          0.026978          0.151377          0.021198   \n",
      "3          0.056431          0.057615          0.055773          0.075030   \n",
      "4          0.071139          0.085607          0.019392         -0.036403   \n",
      "\n",
      "   0throw_9thcolumn  0throw_10thcolumn  ...  Basic_Demos_Enroll_Year  \\\n",
      "0          0.068583           0.029271  ...                   2018.0   \n",
      "1         -0.004750          -0.035073  ...                   2015.0   \n",
      "2          0.083721          -0.014275  ...                   2019.0   \n",
      "3          0.001033          -0.064031  ...                   2017.0   \n",
      "4         -0.020375           0.005426  ...                   2019.0   \n",
      "\n",
      "   Basic_Demos_Study_Site  PreInt_Demos_Fam_Child_Ethnicity  \\\n",
      "0                     1.0                               0.0   \n",
      "1                     1.0                               1.0   \n",
      "2                     1.0                               0.0   \n",
      "3                     1.0                               0.0   \n",
      "4                     1.0                               2.0   \n",
      "\n",
      "   PreInt_Demos_Fam_Child_Race  MRI_Track_Scan_Location  \\\n",
      "0                          1.0                      2.0   \n",
      "1                          8.0                      1.0   \n",
      "2                          0.0                      2.0   \n",
      "3                          0.0                      2.0   \n",
      "4                          8.0                      2.0   \n",
      "\n",
      "   Barratt_Barratt_P1_Edu  Barratt_Barratt_P1_Occ  Barratt_Barratt_P2_Edu  \\\n",
      "0                    21.0                    45.0                    21.0   \n",
      "1                     6.0                     5.0                     0.0   \n",
      "2                    18.0                    35.0                     9.0   \n",
      "3                    21.0                    40.0                    21.0   \n",
      "4                     9.0                    35.0                     0.0   \n",
      "\n",
      "   Barratt_Barratt_P2_Occ  participant_id  \n",
      "0                    45.0    70z8Q2xdTXM3  \n",
      "1                    15.0    WHWymJu6zNZi  \n",
      "2                    20.0    4PAQp1M6EyAo  \n",
      "3                    40.0    obEacy4Of68I  \n",
      "4                     0.0    s7WzzDcmDOhF  \n",
      "\n",
      "[5 rows x 19928 columns]\n",
      "Imputed Test Data:\n",
      "   0throw_1thcolumn  0throw_2thcolumn  0throw_3thcolumn  0throw_4thcolumn  \\\n",
      "0          0.548480          0.713607          0.557319          0.524369   \n",
      "1          0.427740          0.363022          0.402862          0.363003   \n",
      "2          0.139572          0.390106         -0.087041          0.196852   \n",
      "3          0.133561          0.778326          0.416355          0.471840   \n",
      "4          0.126699          0.575446          0.509422          0.363193   \n",
      "\n",
      "   0throw_5thcolumn  0throw_6thcolumn  0throw_7thcolumn  0throw_8thcolumn  \\\n",
      "0          0.693364          0.770032          0.724406          0.390118   \n",
      "1          0.534558          0.345347          0.409471          0.303328   \n",
      "2          0.088148          0.023843          0.381782          0.068979   \n",
      "3          0.568460          0.633660          0.501113          0.345461   \n",
      "4          0.427544          0.449924          0.451796          0.223927   \n",
      "\n",
      "   0throw_9thcolumn  0throw_10thcolumn  ...  Basic_Demos_Enroll_Year  \\\n",
      "0          0.547912           0.470220  ...                   2022.0   \n",
      "1          0.402515           0.275052  ...                   2023.0   \n",
      "2          0.377488           0.301871  ...                   2022.0   \n",
      "3          0.467943           0.417308  ...                   2022.0   \n",
      "4          0.298248           0.304429  ...                   2022.0   \n",
      "\n",
      "   Basic_Demos_Study_Site  PreInt_Demos_Fam_Child_Ethnicity  \\\n",
      "0                     4.0                               0.0   \n",
      "1                     4.0                               0.0   \n",
      "2                     4.0                               0.0   \n",
      "3                     4.0                               0.0   \n",
      "4                     4.0                               2.0   \n",
      "\n",
      "   PreInt_Demos_Fam_Child_Race  MRI_Track_Scan_Location  \\\n",
      "0                          0.0                      4.0   \n",
      "1                          0.0                      4.0   \n",
      "2                          0.0                      4.0   \n",
      "3                          0.0                      3.0   \n",
      "4                          0.0                      4.0   \n",
      "\n",
      "   Barratt_Barratt_P1_Edu  Barratt_Barratt_P1_Occ  Barratt_Barratt_P2_Edu  \\\n",
      "0                    21.0                    30.0               18.000000   \n",
      "1                    21.0                    45.0               18.458955   \n",
      "2                    21.0                    40.0               18.000000   \n",
      "3                    21.0                    45.0               21.000000   \n",
      "4                    18.0                     0.0               21.000000   \n",
      "\n",
      "   Barratt_Barratt_P2_Occ  participant_id  \n",
      "0                    30.0    Cfwaf5FX7jWK  \n",
      "1                    30.0    vhGrzmvA3Hjq  \n",
      "2                    40.0    ULliyEXjy4OV  \n",
      "3                    45.0    LZfeAb1xMtql  \n",
      "4                    45.0    EnFOUv0YK1RG  \n",
      "\n",
      "[5 rows x 19928 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Load the training and test data (update paths as necessary)\n",
    "train_fmri_path = 'widsdatathon2025/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv'  # Replace with actual file path\n",
    "train_quant_path = 'widsdatathon2025/TRAIN/TRAIN_QUANTITATIVE_METADATA.xlsx'  # Replace with actual file path\n",
    "train_cat_path = 'widsdatathon2025/TRAIN/TRAIN_CATEGORICAL_METADATA.xlsx'  # Replace with actual file path\n",
    "\n",
    "test_fmri_path = 'widsdatathon2025/TEST/TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv'  # Replace with actual file path\n",
    "test_quant_path = 'widsdatathon2025/TEST/TEST_QUANTITATIVE_METADATA.xlsx'  # Replace with actual file path\n",
    "test_cat_path = 'widsdatathon2025/TEST/TEST_CATEGORICAL.xlsx'  # Replace with actual file path\n",
    "\n",
    "# Load the datasets\n",
    "train_fmri_data = pd.read_csv(train_fmri_path)\n",
    "train_quant_data = pd.read_excel(train_quant_path)\n",
    "train_cat_data = pd.read_excel(train_cat_path)\n",
    "\n",
    "test_fmri_data = pd.read_csv(test_fmri_path)\n",
    "test_quant_data = pd.read_excel(test_quant_path)\n",
    "test_cat_data = pd.read_excel(test_cat_path)\n",
    "\n",
    "# Step 2: Merge the training data based on 'participant_id' using an inner join\n",
    "train_data = pd.concat([train_fmri_data, train_quant_data, train_cat_data], axis=1)\n",
    "train_data_merged = pd.merge(train_fmri_data, train_quant_data, on='participant_id', how='inner')\n",
    "train_data_merged = pd.merge(train_data_merged, train_cat_data, on='participant_id', how='inner')\n",
    "\n",
    "# Step 3: Merge the test data based on 'participant_id' using an inner join\n",
    "test_data = pd.concat([test_fmri_data, test_quant_data, test_cat_data], axis=1)\n",
    "test_data_merged = pd.merge(test_fmri_data, test_quant_data, on='participant_id', how='inner')\n",
    "test_data_merged = pd.merge(test_data_merged, test_cat_data, on='participant_id', how='inner')\n",
    "\n",
    "\n",
    "# Use SimpleImputer for a faster alternative\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Imputation for training data (excluding participant_id)\n",
    "train_data_imputed = imputer.fit_transform(train_data_merged.drop(columns=['participant_id']))\n",
    "train_data_imputed_df = pd.DataFrame(train_data_imputed, columns=train_data_merged.columns[1:])\n",
    "train_data_imputed_df['participant_id'] = train_data_merged['participant_id']  # Add back the participant_id column\n",
    "\n",
    "# Imputation for test data (excluding participant_id)\n",
    "test_data_imputed = imputer.fit_transform(test_data_merged.drop(columns=['participant_id']))\n",
    "test_data_imputed_df = pd.DataFrame(test_data_imputed, columns=test_data_merged.columns[1:])\n",
    "test_data_imputed_df['participant_id'] = test_data_merged['participant_id']  # Add back the participant_id column\n",
    "\n",
    "# Step 5: Check the imputed training and test data\n",
    "print(\"Imputed Training Data:\")\n",
    "print(train_data_imputed_df.head())\n",
    "\n",
    "print(\"Imputed Test Data:\")\n",
    "print(test_data_imputed_df.head())\n",
    "\n",
    "# Now you have imputed training and test datasets ready for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to processed_train_data.csv\n",
      "Test data saved to processed_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save the training data (assuming `full_train_data` is your processed training data)\n",
    "train_data_file_path = 'processed_train_data.csv'  # Specify the desired file path\n",
    "train_data_imputed_df.to_csv(train_data_file_path, index=False)  # Save without index column\n",
    "\n",
    "print(f\"Training data saved to {train_data_file_path}\")\n",
    "\n",
    "# Step 2: Save the test data (assuming `test_full_data` is your processed test data)\n",
    "test_data_file_path = 'processed_test_data.csv'  # Specify the desired file path\n",
    "test_data_imputed_df.to_csv(test_data_file_path, index=False)  # Save without index column\n",
    "\n",
    "print(f\"Test data saved to {test_data_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this block of code to load in the .csv files of the imputed and merged data and go from here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load in the training data\n",
    "train_data_imputed_df = pd.read_csv('processed_train_data.csv')\n",
    "test_data_imputed_df = pd.read_csv('processed_test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# --- Custom Dataset for Multi-Task Learning ---\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, df, target_cols):\n",
    "        self.y = df[target_cols].values.astype(np.float32)\n",
    "        self.X = df.drop(columns=target_cols + ['participant_id']).values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]  # y is a vector of targets\n",
    "\n",
    "# --- Custom Weighted Loss Function ---\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, target, sex, adhd):\n",
    "        # Assign a weight of 2 for female ADHD cases (Sex_F=1, ADHD_Outcome=1)\n",
    "        weights = torch.ones_like(target[:, 0])  # Default weight is 1, shape (batch_size,)\n",
    "        weights[(sex == 1) & (adhd == 1)] = 2  # Apply weight of 2 for female ADHD cases\n",
    "        \n",
    "        # Compute the loss for each task\n",
    "        loss_sex = nn.BCELoss(reduction='none')(preds[:, 0], target[:, 0])  # Loss for Sex_F\n",
    "        loss_adhd = nn.BCELoss(reduction='none')(preds[:, 1], target[:, 1])  # Loss for ADHD_Outcome\n",
    "        \n",
    "        # Apply the weights to each task's loss\n",
    "        weighted_loss_sex = loss_sex * weights  # Apply weight for Sex_F loss\n",
    "        weighted_loss_adhd = loss_adhd * weights  # Apply weight for ADHD_Outcome loss\n",
    "\n",
    "        return (weighted_loss_sex.mean() + weighted_loss_adhd.mean()) / 2\n",
    "\n",
    "# --- Multi-Task Neural Network ---\n",
    "class MultiTaskNN(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "        # Shared layers with dropout\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(dropout_rate),  # Dropout added here\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(dropout_rate)  # Dropout added here\n",
    "        )\n",
    "        # Separate heads for each task\n",
    "        self.sex_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.adhd_head = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared_rep = self.shared(x)\n",
    "        sex_pred = self.sex_head(shared_rep)\n",
    "        adhd_pred = self.adhd_head(shared_rep)\n",
    "        return torch.cat([sex_pred, adhd_pred], dim=1)  # output size [batch_size, 2]\n",
    "\n",
    "# --- F1 Score Calculation with Weights ---\n",
    "def weighted_f1_score(preds, target, sex, adhd):\n",
    "    weights = np.ones_like(target[:, 0])  # Default weight is 1 for both tasks\n",
    "    weights[(sex == 1) & (adhd == 1)] = 2  # Apply weight of 2 for female ADHD cases\n",
    "    \n",
    "    # Calculate F1 score for both tasks\n",
    "    f1_sex = f1_score(target[:, 0], (preds[:, 0] > 0.5).astype(int), sample_weight=weights)\n",
    "    f1_adhd = f1_score(target[:, 1], (preds[:, 1] > 0.5).astype(int), sample_weight=weights)\n",
    "    \n",
    "    return (f1_sex + f1_adhd) / 2\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_model(df, target_cols, num_epochs=100, batch_size=64, learning_rate=0.0001, weight_decay=0.01, dropout_rate=0.2):\n",
    "    # Set device (this should be defined before using it)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Split the data into training and validation sets (80-20 split)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = MultiTaskDataset(train_df, target_cols)\n",
    "    val_dataset = MultiTaskDataset(val_df, target_cols)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    input_dim = train_df.drop(columns=target_cols + ['participant_id']).shape[1]\n",
    "    model = MultiTaskNN(input_dim, dropout_rate).to(device)\n",
    "    \n",
    "    # Initialize custom weighted loss\n",
    "    criterion = WeightedBCELoss()\n",
    "\n",
    "    # Define optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_f1 = 0  # For tracking best F1 score\n",
    "    model_weights_per_epoch = []  # Store model weights at each epoch\n",
    "    val_f1_scores = []  # Store F1 scores for validation\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            sex = y_batch[:, 0]  # Extract Sex_F values\n",
    "            adhd = y_batch[:, 1]  # Extract ADHD_Outcome values\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            \n",
    "            # Calculate weighted loss\n",
    "            loss = criterion(preds, y_batch, sex, adhd)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(y_batch.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        \n",
    "        # Calculate weighted F1 score for both tasks\n",
    "        f1_score_avg = weighted_f1_score(all_preds, all_targets, all_targets[:, 0], all_targets[:, 1])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Val Weighted F1 Score: {f1_score_avg:.4f}\")\n",
    "        \n",
    "        # Store model weights and validation F1 score\n",
    "        model_weights_per_epoch.append(model.state_dict())  # Save weights\n",
    "        val_f1_scores.append(f1_score_avg)  # Save validation score\n",
    "\n",
    "    # After training, find the epoch with the best validation F1 score\n",
    "    best_epoch = np.argmax(val_f1_scores)  # Get the index of the best F1 score\n",
    "    print(f\"Best epoch: {best_epoch + 1} with F1 Score: {val_f1_scores[best_epoch]:.4f}\")\n",
    "    \n",
    "    # Reload the best model weights\n",
    "    model.load_state_dict(model_weights_per_epoch[best_epoch])  # Load best model weights\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Val Weighted F1 Score: 0.6737\n",
      "Epoch 2/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 3/100 - Val Weighted F1 Score: 0.4863\n",
      "Epoch 4/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 5/100 - Val Weighted F1 Score: 0.5224\n",
      "Epoch 6/100 - Val Weighted F1 Score: 0.4461\n",
      "Epoch 7/100 - Val Weighted F1 Score: 0.7669\n",
      "Epoch 8/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 9/100 - Val Weighted F1 Score: 0.3893\n",
      "Epoch 10/100 - Val Weighted F1 Score: 0.7694\n",
      "Epoch 11/100 - Val Weighted F1 Score: 0.7362\n",
      "Epoch 12/100 - Val Weighted F1 Score: 0.7728\n",
      "Epoch 13/100 - Val Weighted F1 Score: 0.7596\n",
      "Epoch 14/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 15/100 - Val Weighted F1 Score: 0.6120\n",
      "Epoch 16/100 - Val Weighted F1 Score: 0.5614\n",
      "Epoch 17/100 - Val Weighted F1 Score: 0.4365\n",
      "Epoch 18/100 - Val Weighted F1 Score: 0.6440\n",
      "Epoch 19/100 - Val Weighted F1 Score: 0.6903\n",
      "Epoch 20/100 - Val Weighted F1 Score: 0.6838\n",
      "Epoch 21/100 - Val Weighted F1 Score: 0.4072\n",
      "Epoch 22/100 - Val Weighted F1 Score: 0.6289\n",
      "Epoch 23/100 - Val Weighted F1 Score: 0.2804\n",
      "Epoch 24/100 - Val Weighted F1 Score: 0.3281\n",
      "Epoch 25/100 - Val Weighted F1 Score: 0.7821\n",
      "Epoch 26/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 27/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 28/100 - Val Weighted F1 Score: 0.7596\n",
      "Epoch 29/100 - Val Weighted F1 Score: 0.7596\n",
      "Epoch 30/100 - Val Weighted F1 Score: 0.3947\n",
      "Epoch 31/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 32/100 - Val Weighted F1 Score: 0.2660\n",
      "Epoch 33/100 - Val Weighted F1 Score: 0.7604\n",
      "Epoch 34/100 - Val Weighted F1 Score: 0.0392\n",
      "Epoch 35/100 - Val Weighted F1 Score: 0.7596\n",
      "Epoch 36/100 - Val Weighted F1 Score: 0.7593\n",
      "Epoch 37/100 - Val Weighted F1 Score: 0.4384\n",
      "Epoch 38/100 - Val Weighted F1 Score: 0.7727\n",
      "Epoch 39/100 - Val Weighted F1 Score: 0.6487\n",
      "Epoch 40/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 41/100 - Val Weighted F1 Score: 0.7721\n",
      "Epoch 42/100 - Val Weighted F1 Score: 0.7317\n",
      "Epoch 43/100 - Val Weighted F1 Score: 0.3631\n",
      "Epoch 44/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 45/100 - Val Weighted F1 Score: 0.7728\n",
      "Epoch 46/100 - Val Weighted F1 Score: 0.3281\n",
      "Epoch 47/100 - Val Weighted F1 Score: 0.7361\n",
      "Epoch 48/100 - Val Weighted F1 Score: 0.6553\n",
      "Epoch 49/100 - Val Weighted F1 Score: 0.4235\n",
      "Epoch 50/100 - Val Weighted F1 Score: 0.4451\n",
      "Epoch 51/100 - Val Weighted F1 Score: 0.4924\n",
      "Epoch 52/100 - Val Weighted F1 Score: 0.0134\n",
      "Epoch 53/100 - Val Weighted F1 Score: 0.7906\n",
      "Epoch 54/100 - Val Weighted F1 Score: 0.7015\n",
      "Epoch 55/100 - Val Weighted F1 Score: 0.4515\n",
      "Epoch 56/100 - Val Weighted F1 Score: 0.4316\n",
      "Epoch 57/100 - Val Weighted F1 Score: 0.5429\n",
      "Epoch 58/100 - Val Weighted F1 Score: 0.7999\n",
      "Epoch 59/100 - Val Weighted F1 Score: 0.4873\n",
      "Epoch 60/100 - Val Weighted F1 Score: 0.8083\n",
      "Epoch 61/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 62/100 - Val Weighted F1 Score: 0.4364\n",
      "Epoch 63/100 - Val Weighted F1 Score: 0.7634\n",
      "Epoch 64/100 - Val Weighted F1 Score: 0.7613\n",
      "Epoch 65/100 - Val Weighted F1 Score: 0.7631\n",
      "Epoch 66/100 - Val Weighted F1 Score: 0.3352\n",
      "Epoch 67/100 - Val Weighted F1 Score: 0.3864\n",
      "Epoch 68/100 - Val Weighted F1 Score: 0.7726\n",
      "Epoch 69/100 - Val Weighted F1 Score: 0.1947\n",
      "Epoch 70/100 - Val Weighted F1 Score: 0.7280\n",
      "Epoch 71/100 - Val Weighted F1 Score: 0.4303\n",
      "Epoch 72/100 - Val Weighted F1 Score: 0.7933\n",
      "Epoch 73/100 - Val Weighted F1 Score: 0.6816\n",
      "Epoch 74/100 - Val Weighted F1 Score: 0.5898\n",
      "Epoch 75/100 - Val Weighted F1 Score: 0.4391\n",
      "Epoch 76/100 - Val Weighted F1 Score: 0.5353\n",
      "Epoch 77/100 - Val Weighted F1 Score: 0.0267\n",
      "Epoch 78/100 - Val Weighted F1 Score: 0.7921\n",
      "Epoch 79/100 - Val Weighted F1 Score: 0.5661\n",
      "Epoch 80/100 - Val Weighted F1 Score: 0.0000\n",
      "Epoch 81/100 - Val Weighted F1 Score: 0.3296\n",
      "Epoch 82/100 - Val Weighted F1 Score: 0.4645\n",
      "Epoch 83/100 - Val Weighted F1 Score: 0.6914\n",
      "Epoch 84/100 - Val Weighted F1 Score: 0.3281\n",
      "Epoch 85/100 - Val Weighted F1 Score: 0.2652\n",
      "Epoch 86/100 - Val Weighted F1 Score: 0.7763\n",
      "Epoch 87/100 - Val Weighted F1 Score: 0.7648\n",
      "Epoch 88/100 - Val Weighted F1 Score: 0.7774\n",
      "Epoch 89/100 - Val Weighted F1 Score: 0.4370\n",
      "Epoch 90/100 - Val Weighted F1 Score: 0.4357\n",
      "Epoch 91/100 - Val Weighted F1 Score: 0.3273\n",
      "Epoch 92/100 - Val Weighted F1 Score: 0.5760\n",
      "Epoch 93/100 - Val Weighted F1 Score: 0.0920\n",
      "Epoch 94/100 - Val Weighted F1 Score: 0.4366\n",
      "Epoch 95/100 - Val Weighted F1 Score: 0.5602\n",
      "Epoch 96/100 - Val Weighted F1 Score: 0.6990\n",
      "Epoch 97/100 - Val Weighted F1 Score: 0.7759\n",
      "Epoch 98/100 - Val Weighted F1 Score: 0.5761\n",
      "Epoch 99/100 - Val Weighted F1 Score: 0.4389\n",
      "Epoch 100/100 - Val Weighted F1 Score: 0.3487\n",
      "Best epoch: 60 with F1 Score: 0.8083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load the target data from a separate .xlsx file ---\n",
    "target_file_path = 'widsdatathon2025/TRAIN/TRAINING_SOLUTIONS.xlsx' \n",
    "target_data = pd.read_excel(target_file_path)\n",
    "\n",
    "# Merging the target columns with the training data\n",
    "train_data_with_targets = pd.merge(train_data_imputed_df, target_data, on='participant_id', how='inner')\n",
    "\n",
    "\n",
    "# Define target columns for ADHD and Sex\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(train_data_with_targets, target_cols, num_epochs=100, batch_size=32, learning_rate=0.0001, weight_decay=0.01, dropout_rate=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting ADHD and Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def predict_and_save_csv(trained_model, test_df, device, output_path='test_predictions.csv'):\n",
    "    # Step 1: Preprocess the test data (same as training preprocessing)\n",
    "    X_test = test_df.drop(columns=['participant_id'])  # Features only\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce')  # Ensure all data is numeric\n",
    "    \n",
    "    # Handle any NaNs in test data by filling with the mean (or other strategy)\n",
    "    X_test.fillna(X_test.mean(), inplace=True)\n",
    "\n",
    "    # Convert to torch tensor and move to the appropriate device (CPU or GPU)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Step 2: Make predictions\n",
    "    trained_model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(X_test_tensor)\n",
    "        \n",
    "        # Outputs will be [batch_size, 2], where outputs[:, 0] corresponds to Sex_F, and outputs[:, 1] corresponds to ADHD_Outcome\n",
    "        predictions = (outputs.cpu().numpy() > 0.5).astype(int)  # Convert to 0 or 1 for both targets\n",
    "    \n",
    "    # Step 3: Prepare the output DataFrame with participant_id, ADHD_Outcome, and Sex_F\n",
    "    results = pd.DataFrame(predictions, columns=['Sex_F', 'ADHD_Outcome'])\n",
    "    \n",
    "    # Ensure participant_id is a 1D array, not 2D\n",
    "    results['participant_id'] = test_df['participant_id'].values  # Ensure participant_id is a 1D array\n",
    "    \n",
    "    # Step 4: Reorder the columns to match the desired format\n",
    "    results = results[['participant_id', 'ADHD_Outcome', 'Sex_F']]\n",
    "    \n",
    "    # Step 5: Save the predictions to a CSV file\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Assuming `trained_model` is your trained model and `test_data` is your test DataFrame\n",
    "# And `device` is either 'cuda' (GPU) or 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predict_and_save_csv(trained_model, test_data_imputed_df, device, output_path='test_predictions.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just me messing around with different models to see what happens. Ignore this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 960 candidates, totalling 2880 fits\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/52/v1gpfmxd54d6bqwl7msx6jb40000gn/T/ipykernel_72410/3241726433.py\", line 79, in <module>\n",
      "    grid_result = grid.fit(X_train, y_train)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    This mixin is empty, and only exists to indicate that the estimator is a\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 898, in fit\n",
      "    Training vectors, where `n_samples` is the number of samples and\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 1422, in _run_search\n",
      "    above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py\", line 845, in evaluate_candidates\n",
      "    \"\"\"Get the scorer(s) to be used.\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    results : list\n",
      "           ^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    if config is None:\n",
      "               ^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 717, in _fit_and_score\n",
      "    split_progress=None,\n",
      "                    ^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 230, in set_params\n",
      "ValueError: Invalid parameter 'learning_rate' for estimator KerasModel(dropout_rate=0.1). Valid parameters are: ['activation', 'dropout_rate', 'optimizer', 'weight_decay'].\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "                                              ^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/core.py\", line 720, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/core.py\", line 667, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/stack_data/core.py\", line 646, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# This doesn't work, something about keras that I don't understand \n",
    "# Also this code can only be ran one at a time because it's not picklable (new word of the day for me)\n",
    "# Trying again with PyTorch - see next code block\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Assuming you have already imputed the data (train_imputed_data, test_imputed_data, target_columns)\n",
    "\n",
    "# Step 1: Separate features and targets for training\n",
    "X_train = train_data_imputed_df.drop(columns=['participant_id'])  # Remove participant_id for training\n",
    "y_train = train_data_with_targets[target_cols]  # Targets for training (ADHD_Outcome, Sex_F)\n",
    "\n",
    "# Step 2: Separate features for test data\n",
    "X_test = test_data_imputed_df.drop(columns=['participant_id'])\n",
    "participant_id_test = test_data_imputed_df['participant_id']\n",
    "\n",
    "# Step 3: Create the model function\n",
    "def create_model(optimizer='adam', activation='relu', dropout_rate=0.2, weight_decay=1e-4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=X_train.shape[1], activation=activation))  # Input layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer\n",
    "    model.add(Dense(64, activation=activation))  # Hidden layer\n",
    "    model.add(Dropout(dropout_rate))  # Dropout layer\n",
    "    model.add(Dense(2, activation='sigmoid'))  # Output layer (binary classification for both ADHD_Outcome and Sex_F)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Custom estimator class to use TensorFlow with GridSearchCV\n",
    "class KerasModel(BaseEstimator):\n",
    "    def __init__(self, optimizer='adam', activation='relu', dropout_rate=0.2, weight_decay=1e-4):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, batch_size=32, epochs=50):\n",
    "        self.model = create_model(\n",
    "            optimizer=self.optimizer,\n",
    "            activation=self.activation,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(int)  # Binary classification (0 or 1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.evaluate(X, y, verbose=0)[1]  # Return accuracy\n",
    "\n",
    "# Step 4: Create the model for GridSearchCV\n",
    "model = KerasModel()\n",
    "\n",
    "# Step 5: Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop', 'adagrad'],  # Different optimizers\n",
    "    'activation': ['relu', 'tanh', 'sigmoid', 'swish'],  # Different activation functions\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.4, 0.5],  # Dropout rates for regularization\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5, 1e-6],  # Learning rates\n",
    "    'weight_decay': [0, 1e-4, 1e-3],  # Weight decay for L2 regularization\n",
    "}\n",
    "\n",
    "# Step 6: Create a custom scoring function\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Step 7: Use GridSearchCV to search the best hyperparameters with `n_jobs=1`\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3, verbose=2, scoring=scorer)\n",
    "\n",
    "# Step 8: Fit the grid search on your training data\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Get the best hyperparameters and print\n",
    "best_params = grid_result.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 10: Create the final model using the best hyperparameters\n",
    "final_model = KerasModel(\n",
    "    optimizer=best_params['optimizer'],\n",
    "    activation=best_params['activation'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "\n",
    "# Step 11: Train the final model using the best hyperparameters\n",
    "final_model.fit(X_train, y_train, batch_size=32, epochs=50)  # Default batch_size and epochs\n",
    "\n",
    "# Step 12: Make predictions on the test set\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Step 13: Prepare the output DataFrame with participant_id, ADHD_Outcome, and Sex_F\n",
    "results = pd.DataFrame(predictions, columns=['ADHD_Outcome', 'Sex_F'])\n",
    "results['participant_id'] = participant_id_test.values\n",
    "\n",
    "# Step 14: Reorder the columns\n",
    "results = results[['participant_id', 'ADHD_Outcome', 'Sex_F']]\n",
    "\n",
    "# Step 15: Save the results to a CSV file\n",
    "results.to_csv('test_predictions_model2.csv', index=False)\n",
    "print(\"Predictions saved to 'test_predictions_model2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not NOT working, I just haven't finessed it in any way. It's a straight copy from chatgpt\n",
    "# I am going to bed though so I will try again tomorrow to actually make it fit in with the data\n",
    "# To fix it involves changing y_test into something that can be predicted\n",
    "# Similar to what I have above with the keras model but, like, better :)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your preprocessed data (this step is assumed to be done)\n",
    "# Assuming train_data_imputed_df and test_data_imputed_df are ready, along with target_cols\n",
    "# Let's separate features and targets for training\n",
    "X_train = train_data_imputed_df.drop(columns=['participant_id'])\n",
    "y_train = target_cols  # Targets for training (ADHD_Outcome, Sex_F)\n",
    "\n",
    "X_test = test_data_imputed_df.drop(columns=['participant_id'])\n",
    "y_test = target_cols  # Assuming test data has the same target columns for evaluation\n",
    "\n",
    "# Step 2: Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.2, activation='relu'):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            self.activation = torch.relu  # default\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid output for binary classification\n",
    "\n",
    "# Step 3: Define the function to train and evaluate the model\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, \n",
    "                              optimizer_name='adam', activation='relu', \n",
    "                              dropout_rate=0.2, batch_size=32, epochs=50, learning_rate=1e-3):\n",
    "    # Prepare data\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = NeuralNetwork(input_dim=X_train.shape[1], output_dim=y_train.shape[1], \n",
    "                          dropout_rate=dropout_rate, activation=activation)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Default to Adam\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # No need to calculate gradients for evaluation\n",
    "        test_predictions = model(X_test_tensor)\n",
    "        predicted_classes = (test_predictions > 0.5).float()  # Apply threshold\n",
    "        accuracy = accuracy_score(y_test_tensor.numpy(), predicted_classes.numpy())\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Define hyperparameter search space\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 30, 50],\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "}\n",
    "\n",
    "# Step 5: Loop through the hyperparameters and train the models\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for optimizer in param_grid['optimizer']:\n",
    "    for activation in param_grid['activation']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for batch_size in param_grid['batch_size']:\n",
    "                for epochs in param_grid['epochs']:\n",
    "                    for learning_rate in param_grid['learning_rate']:\n",
    "                        # Train and evaluate the model with these hyperparameters\n",
    "                        accuracy = train_and_evaluate_model(\n",
    "                            X_train, y_train, X_test, y_test,\n",
    "                            optimizer_name=optimizer, activation=activation, \n",
    "                            dropout_rate=dropout_rate, batch_size=batch_size, \n",
    "                            epochs=epochs, learning_rate=learning_rate\n",
    "                        )\n",
    "                        print(f\"Tested {optimizer}, {activation}, {dropout_rate}, {batch_size}, {epochs}, {learning_rate} -> Accuracy: {accuracy}\")\n",
    "                        \n",
    "                        # Track the best model\n",
    "                        if accuracy > best_accuracy:\n",
    "                            best_accuracy = accuracy\n",
    "                            best_params = {\n",
    "                                'optimizer': optimizer,\n",
    "                                'activation': activation,\n",
    "                                'dropout_rate': dropout_rate,\n",
    "                                'batch_size': batch_size,\n",
    "                                'epochs': epochs,\n",
    "                                'learning_rate': learning_rate\n",
    "                            }\n",
    "\n",
    "# Step 6: Print best hyperparameters and accuracy\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
